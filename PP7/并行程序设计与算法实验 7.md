# **并行程序设计与算法实验 7**

| 实验  | MPI并行应用            | 专业     | 计算机科学与技术 |
| ----- | ---------------------- | -------- | ---------------- |
| 学号  | 21311525               | 姓名     | 马梓培           |
| Email | mazp@mail2.sysu.edu.cn | 完成日期 | 2024/05/14       |

## 1. 实验要求

> 1. 使用MPI对快速傅里叶变换进行并行化
>	- 阅读参考文献中的串行傅里叶变换代码(fft_serial.cpp)
> 	- 使用MPI对其进行并行化（可能需要对原代码进行调整）
>	- 通过数据打包优化消息传递效率
>		- MPI_Pack/MPI-Unpack或MPI_Type_create_struct对数据重组
>	- 性能分析
>		- 分析不同并行规模（进程数）及问题规模（N）下的性能
>		- 分析数据打包对于并行程序性能的影响
>		- 使用valgrind massif工具集采集并分析程序的内存消耗




## 2. 实验原理

部分知识借鉴自[知乎](https://zhuanlan.zhihu.com/p/267276132)/[蝴蝶操作](https://zhuanlan.zhihu.com/p/473327863)。

**快速傅里叶变换(FFT)** 是一种快速有效率的对 **DFT** 的实现。**FFT** 加速多项式乘法，其基本思想是将两个多项式的系数表示通过 **FFT** 转化为特殊点处的点值表示，然后计算两个多项式点值表示的乘积得到原多项式卷积的点值表示，再将多项式卷积的点值表示进行 **逆离散傅里叶变换(IDFT)** 就得到了乘积多项式的系数表示。

**蝶形操作**的原理是通过将输入的序列分成两个部分，然后对这两个部分进行运算得到输出序列。具体来说，对于一个长度为N的复数序列，蝶形运算将其分成两个长度为N/2的子序列。这个过程递归进行，直到序列长度为1，此时每个元素就是其自己的DFT。

在蝶形运算中，旋转因子是关键要素。旋转因子是一个复数，其幅值和相位分别表示序列中相邻数据点的幅度差异和相位差异。每个蝶形运算单元由两个输入和两个输出组成，通常使用复数乘法和加法来实现。具体来说，输入两个复数a和b，然后执行以下操作：

1. 乘法：计算c = a * W，其中W是旋转因子，可以表示为W = exp(-2πi/N)，N是输入序列的长度。
2. 加法：计算输出序列的两个元素，分别为p = a + b和q = a - b。

最终，输出序列的两个元素p和q就是经过蝶形运算后的结果。

## 3. 实现代码

代码见附件或[github](https://github.com/xiao10ma/Parallel-Programming/tree/master/PP7).

### 3.1 串行step函数

本实验主要是对step函数的并行化。我先来解释一下串行的step函数，它是用蝴蝶操作来实现的。

```cpp
void step ( int n, int mj, double a[], double b[], double c[],
  double d[], double w[], double sgn ) {
  double ambr;
  double ambu;
  int j;
  int ja;
  int jb;
  int jc;
  int jd;
  int jw;
  int k;
  int lj;
  int mj2;
  double wjw[2];

  mj2 = 2 * mj;
  lj = n / mj2;

  for ( j = 0; j < lj; j++ )
  {
    jw = j * mj;
    ja  = jw;
    jb  = ja;
    jc  = j * mj2;
    jd  = jc;

    wjw[0] = w[jw*2+0]; 
    wjw[1] = w[jw*2+1];

    if ( sgn < 0.0 ) 
    {
      wjw[1] = - wjw[1];
    }

    for ( k = 0; k < mj; k++ )
    {
      c[(jc+k)*2+0] = a[(ja+k)*2+0] + b[(jb+k)*2+0];
      c[(jc+k)*2+1] = a[(ja+k)*2+1] + b[(jb+k)*2+1];

      ambr = a[(ja+k)*2+0] - b[(jb+k)*2+0];
      ambu = a[(ja+k)*2+1] - b[(jb+k)*2+1];

      d[(jd+k)*2+0] = wjw[0] * ambr - wjw[1] * ambu;
      d[(jd+k)*2+1] = wjw[1] * ambr + wjw[0] * ambu;
    }
  }
  return;
}
```



**参数解释**

- `n`：数据的长度。
- `mj`：当前步骤的子问题大小。
- `a[]`, `b[]`, `c[]`, `d[]`：输入和输出数组。`a`和`b`是输入数组，`c`和`d`是输出数组。
- `w[]`：旋转因子数组（twiddle factors）。
- `sgn`：符号，用于区分正向和逆向变换。

**主要步骤**

1. **初始化变量**：

   ```cpp
   mj2 = 2 * mj;
   lj = n / mj2;
   ```

2. **外层循环**：

   ```cpp
   for ( j = 0; j < lj; j++ )
   ```

   这个循环遍历了所有的蝴蝶操作组。

3. **计算索引**：

   ```cpp
   jw = j * mj;
   ja  = jw;
   jb  = ja;
   jc  = j * mj2;
   jd  = jc;
   ```

4. **读取旋转因子**：

   ```cpp
   wjw[0] = w[jw*2+0];
   wjw[1] = w[jw*2+1];
   if ( sgn < 0.0 )
   {
     wjw[1] = - wjw[1];
   }
   ```

   这里读取当前组的旋转因子，并根据变换方向（正向或逆向）调整符号。

5. **内层循环（蝴蝶操作）**：

   ```cpp
   for ( k = 0; k < mj; k++ )
   ```

   这个循环进行具体的蝴蝶操作。

6. **蝴蝶计算**：

   ```cpp
   c[(jc+k)*2+0] = a[(ja+k)*2+0] + b[(jb+k)*2+0];
   c[(jc+k)*2+1] = a[(ja+k)*2+1] + b[(jb+k)*2+1];
   
   ambr = a[(ja+k)*2+0] - b[(jb+k)*2+0];
   ambu = a[(ja+k)*2+1] - b[(jb+k)*2+1];
   
   d[(jd+k)*2+0] = wjw[0] * ambr - wjw[1] * ambu;
   d[(jd+k)*2+1] = wjw[1] * ambr + wjw[0] * ambu;
   ```

   这里进行了蝴蝶运算，正如在**2.  实验原理**中所说，包括计算和差以及旋转因子的应用。

通过多次调用`step`函数并逐步减小`mj`，可以完成整个FFT的计算。

### 3.2 并行step函数

```cpp
// x和y的长度是2n，n个复数
void step(int n, int mj, double a[], double b[], double c[], double d[], double w[], double sgn)
{
    double ambr;
    double ambu;
    int j;
    int ja;
    int jb;
    int jc;
    int jd;
    int jw;
    int k;
    int lj;
    int mj2;
    double wjw[2];

    mj2 = 2 * mj;
    lj = n / mj2;

    // 每个进程需要处理的复数的数量
    int size = (lj % comm_sz == 0) ? (lj / comm_sz) : (lj / comm_sz + 1);
    // 每个进程开始处理的复数的位置
    int startlocal = (my_rank * size > lj) ? lj : my_rank * size;
    // 每个进程结束处理的复数的位置（取不到的）
    int endlocal = ((my_rank + 1) * size > lj) ? lj : (my_rank + 1) * size;

    if (my_rank == 0)
    {
        for (int i = 1; i < comm_sz; i++)
        {
            int start = (i * size > lj) ? lj : i * size;
            int end = ((i + 1) * size) > lj ? lj : (i + 1) * size;
            MPI_Send(&a[start * mj2], (end - start) * mj2, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
            MPI_Send(&b[start * mj2], (end - start) * mj2, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
            MPI_Send(&w[start * mj2], (end - start) * mj2, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
        }
    }
    else
    {
        MPI_Recv(&a[startlocal * mj2], (endlocal - startlocal) * mj2, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        MPI_Recv(&b[startlocal * mj2], (endlocal - startlocal) * mj2, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        MPI_Recv(&w[startlocal * mj2], (endlocal - startlocal) * mj2, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    }

    // 计算
    for (j = startlocal; j < endlocal; j++)
    {
        jw = j * mj;
        ja = jw;
        jb = ja;
        jc = j * mj2;
        jd = jc;

        wjw[0] = w[jw * 2 + 0];
        wjw[1] = w[jw * 2 + 1];

        if (sgn < 0.0)
        {
            wjw[1] = -wjw[1];
        }

        for (k = 0; k < mj; k++)
        {
            c[(jc + k) * 2 + 0] = a[(ja + k) * 2 + 0] + b[(jb + k) * 2 + 0];
            c[(jc + k) * 2 + 1] = a[(ja + k) * 2 + 1] + b[(jb + k) * 2 + 1];
            ambr = a[(ja + k) * 2 + 0] - b[(jb + k) * 2 + 0];
            ambu = a[(ja + k) * 2 + 1] - b[(jb + k) * 2 + 1];
            d[(jd + k) * 2 + 0] = wjw[0] * ambr - wjw[1] * ambu;
            d[(jd + k) * 2 + 1] = wjw[1] * ambr + wjw[0] * ambu;
        }
    }

    if (my_rank == 0)
    {
        for (int i = 1; i < comm_sz; i++)
        {
            int start = (i * size > lj) ? lj : i * size;
            int end = ((i + 1) * size) > lj ? lj : (i + 1) * size;
            if (start == end)
                continue;
            MPI_Recv(&c[start * mj2 * 2], (end - start - 1) * mj2 * 2 + 2 * mj, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
            MPI_Recv(&d[start * mj2 * 2], (end - start - 1) * mj2 * 2 + 2 * mj, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        }
    }
    else if (endlocal - startlocal > 0)
    {
        MPI_Send(&c[startlocal * mj2 * 2], (endlocal - startlocal - 1) * mj2 * 2 + 2 * mj, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
        MPI_Send(&d[startlocal * mj2 * 2], (endlocal - startlocal - 1) * mj2 * 2 + 2 * mj, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
    }

    return;
}
```

**参数解释**

与串行类似，不赘述。

**主要步骤**

1. **初始化变量**：

   ```cpp
   mj2 = 2 * mj;
   lj = n / mj2;
   ```

   `mj2`是当前步骤子问题大小的两倍，`lj`是当前步骤中需要处理的子问题的数量。

2. **计算每个进程处理的复数数量及其开始和结束位置**：

   ```cpp
   int size = (lj % comm_sz == 0) ? (lj / comm_sz) : (lj / comm_sz + 1);
   int startlocal = (my_rank * size > lj) ? lj : my_rank * size;
   int endlocal = ((my_rank + 1) * size > lj) ? lj : (my_rank + 1) * size;
   ```

3. **数据分发**：

   - 主进程（`my_rank == 0`）将数据分发给其他进程：

     ```cpp
     if (my_rank == 0)
     {
         for (int i = 1; i < comm_sz; i++)
         {
             int start = (i * size > lj) ? lj : i * size;
             int end = ((i + 1) * size) > lj ? lj : (i + 1) * size;
             MPI_Send(&a[start * mj2], (end - start) * mj2, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
             MPI_Send(&b[start * mj2], (end - start) * mj2, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
             MPI_Send(&w[start * mj2], (end - start) * mj2, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
         }
     }
     else
     {
         MPI_Recv(&a[startlocal * mj2], (endlocal - startlocal) * mj2, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
         MPI_Recv(&b[startlocal * mj2], (endlocal - startlocal) * mj2, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
         MPI_Recv(&w[startlocal * mj2], (endlocal - startlocal) * mj2, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
     }
     ```

4. **并行计算**：

   - 每个进程在其分配的范围内进行蝴蝶计算：

     ```cpp
     for (j = startlocal; j < endlocal; j++)
     {
         jw = j * mj;
         ja = jw;
         jb = ja;
         jc = j * mj2;
         jd = jc;
     
         wjw[0] = w[jw * 2 + 0];
         wjw[1] = w[jw * 2 + 1];
     
         if (sgn < 0.0)
         {
             wjw[1] = -wjw[1];
         }
     
         for (k = 0; k < mj; k++)
         {
             c[(jc + k) * 2 + 0] = a[(ja + k) * 2 + 0] + b[(jb + k) * 2 + 0];
             c[(jc + k) * 2 + 1] = a[(ja + k) * 2 + 1] + b[(jb + k) * 2 + 1];
             ambr = a[(ja + k) * 2 + 0] - b[(jb + k) * 2 + 0];
             ambu = a[(ja + k) * 2 + 1] - b[(jb + k) * 2 + 1];
             d[(jd + k) * 2 + 0] = wjw[0] * ambr - wjw[1] * ambu;
             d[(jd + k) * 2 + 1] = wjw[1] * ambr + wjw[0] * ambu;
         }
     }
     ```

5. **收集结果**：

   - 主进程（`my_rank == 0`）收集其他进程的计算结果：

     ```cpp
     if (my_rank == 0)
     {
         for (int i = 1; i < comm_sz; i++)
         {
             int start = (i * size > lj) ? lj : i * size;
             int end = ((i + 1) * size) > lj ? lj : (i + 1) * size;
             if (start == end)
                 continue;
             MPI_Recv(&c[start * mj2 * 2], (end - start - 1) * mj2 * 2 + 2 * mj, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
             MPI_Recv(&d[start * mj2 * 2], (end - start - 1) * mj2 * 2 + 2 * mj, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
         }
     }
     else if (endlocal - startlocal > 0)
     {
         MPI_Send(&c[startlocal * mj2 * 2], (endlocal - startlocal - 1) * mj2 * 2 + 2 * mj, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
         MPI_Send(&d[startlocal * mj2 * 2], (endlocal - startlocal - 1) * mj2 * 2 + 2 * mj, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);
     }
     ```

简单来说，主进程（`my_rank == 0`）负责数据的分发和结果的收集，而其他进程则负责各自分配到的数据范围内的计算。这样通过并行处理可以有效地加速FFT的计算过程。

### 3.3 数据打包优化消息传递效率

#### 3.3.1 数据打包

在主进程中，数据打包的部分如下：

```cpp
if (my_rank == 0)
{
    for (int i = 1; i < comm_sz; i++)
    {
        int start = (i * size > lj) ? lj : i * size;
        int end = ((i + 1) * size) > lj ? lj : (i + 1) * size;
        int count = (end - start) * mj2;
        double sendbuf[count * 3]; // 三个数组的数据都放在这个缓冲区中
        int position = 0;
        MPI_Pack(&a[start * mj2], count, MPI_DOUBLE, sendbuf, count * 3 * sizeof(MPI_DOUBLE), &position, MPI_COMM_WORLD);
        MPI_Pack(&b[start * mj2], count, MPI_DOUBLE, sendbuf, count * 3 * sizeof(MPI_DOUBLE), &position, MPI_COMM_WORLD);
        MPI_Pack(&w[start * mj2], count, MPI_DOUBLE, sendbuf, count * 3 * sizeof(MPI_DOUBLE), &position, MPI_COMM_WORLD);
        MPI_Send(sendbuf, position, MPI_PACKED, i, 0, MPI_COMM_WORLD);
    }
}

```

在这个部分，主进程对每个从进程执行以下步骤：

1. 计算每个从进程需要处理的起始和结束位置。
2. 计算需要打包的数据量（`count`）。
3. 分配一个缓冲区`sendbuf`，用于存放打包的数据。缓冲区大小为`count * 3`，因为我们要打包三个数组的数据。
4. 使用`MPI_Pack`函数，将数组`a`、`b`和`w`的数据依次打包到缓冲区`sendbuf`中。`position`变量用于记录当前打包位置。
5. 使用`MPI_Send`函数，将打包后的数据发送给对应的从进程。

#### 3.3.2 数据解包

在子进程中，数据解包的部分如下：

```cpp
else
{
    int count = (endlocal - startlocal) * mj2;
    double recvbuf[count * 3]; // 接收缓冲区，包含三个数组的数据
    MPI_Recv(recvbuf, count * 3 * sizeof(MPI_DOUBLE), MPI_PACKED, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    int position = 0;
    MPI_Unpack(recvbuf, count * 3 * sizeof(MPI_DOUBLE), &position, &a[startlocal * mj2], count, MPI_DOUBLE, MPI_COMM_WORLD);
    MPI_Unpack(recvbuf, count * 3 * sizeof(MPI_DOUBLE), &position, &b[startlocal * mj2], count, MPI_DOUBLE, MPI_COMM_WORLD);
    MPI_Unpack(recvbuf, count * 3 * sizeof(MPI_DOUBLE), &position, &w[startlocal * mj2], count, MPI_DOUBLE, MPI_COMM_WORLD);
}

```

在这个部分，从进程执行以下步骤：

1. 计算需要接收的数据量（`count`）。
2. 分配一个接收缓冲区`recvbuf`，用于存放接收到的打包数据。缓冲区大小为`count * 3`。
3. 使用`MPI_Recv`函数，从主进程接收打包的数据，存放到缓冲区`recvbuf`中。
4. 使用`MPI_Unpack`函数，将缓冲区`recvbuf`中的数据依次解包到数组`a`、`b`和`w`中。`position`变量用于记录当前解包位置。

#### 3.3.3 结果打包与解包

最后在结果的打包和解包部分，主进程收集从进程的计算结果：

```cpp
if (my_rank == 0)
{
    for (int i = 1; i < comm_sz; i++)
    {
        int start = (i * size > lj) ? lj : i * size;
        int end = ((i + 1) * size) > lj ? lj : (i + 1) * size;
        if (start == end)
            continue;
        int count = (end - start - 1) * mj2 * 2 + 2 * mj;
        double recvbuf[count * 2];
        MPI_Recv(recvbuf, count * 2 * sizeof(MPI_DOUBLE), MPI_PACKED, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
        int position = 0;
        MPI_Unpack(recvbuf, count * 2 * sizeof(MPI_DOUBLE), &position, &c[start * mj2 * 2], count, MPI_DOUBLE, MPI_COMM_WORLD);
        MPI_Unpack(recvbuf, count * 2 * sizeof(MPI_DOUBLE), &position, &d[start * mj2 * 2], count, MPI_DOUBLE, MPI_COMM_WORLD);
    }
}

```

从进程将计算结果打包并发送给主进程：

```cpp
else if (endlocal - startlocal > 0)
{
    int count = (endlocal - startlocal - 1) * mj2 * 2 + 2 * mj;
    double sendbuf[count * 2];
    int position = 0;
    MPI_Pack(&c[startlocal * mj2 * 2], count, MPI_DOUBLE, sendbuf, count * 2 * sizeof(MPI_DOUBLE), &position, MPI_COMM_WORLD);
    MPI_Pack(&d[startlocal * mj2 * 2], count, MPI_DOUBLE, sendbuf, count * 2 * sizeof(MPI_DOUBLE), &position, MPI_COMM_WORLD);
    MPI_Send(sendbuf, position, MPI_PACKED, 0, 0, MPI_COMM_WORLD);
}

```

这个部分与之前的数据打包和解包类似，只是数据的来源和去向不同。主进程从从进程接收数据并解包，从进程将数据打包并发送给主进程。

通过打包和解包，代码减少了MPI通信中的消息数量，提高了传输效率。打包后的数据量更大，但发送和接收的次数减少，减少了通信开销。

## 4. 实验结果

1. 串行fft：

```bash
xiaoma@xiaoma-virtual-machine:~/Parallel-Programming/PP7$ ./fft_serial.out 
14 May 2024 10:46:31 AM

FFT_SERIAL
  C++ version

  Demonstrate an implementation of the Fast Fourier Transform
  of a complex data vector.

  Accuracy check:

    FFT ( FFT ( X(1:N) ) ) == N * X(1:N)

             N      NITS    Error         Time          Time/Call     MFLOPS

             2     10000   7.85908e-17      0.000715     3.575e-08        279.72
             4     10000   1.20984e-16      0.001738      8.69e-08       460.299
             8     10000    6.8208e-17      0.003296     1.648e-07       728.155
            16     10000   1.43867e-16      0.008013    4.0065e-07       798.702
            32      1000   1.33121e-16      0.001862      9.31e-07       859.291
            64      1000   1.77654e-16      0.004603    2.3015e-06       834.239
           128      1000   1.92904e-16      0.010039    5.0195e-06       892.519
           256      1000   2.09232e-16      0.024331   1.21655e-05       841.725
           512       100   1.92749e-16       0.00476      2.38e-05       968.067
          1024       100   2.30861e-16      0.010597    5.2985e-05       966.311
          2048       100   2.44762e-16      0.022106    0.00011053       1019.09
          4096       100   2.47978e-16      0.048901   0.000244505       1005.13
          8192        10   2.57809e-16      0.010543    0.00052715       1010.11
         16384        10   2.73399e-16      0.023486     0.0011743        976.65
         32768        10   2.92301e-16      0.047772     0.0023886       1028.89
         65536        10   2.82993e-16       0.10281     0.0051405       1019.92
        131072         1   3.14967e-16      0.021104      0.010552       1055.83
        262144         1    3.2186e-16      0.044271     0.0221355       1065.84
        524288         1   3.28137e-16       0.09222       0.04611       1080.19
       1048576         1    3.2859e-16      0.203073      0.101537       1032.71

FFT_SERIAL:
  Normal end of execution.

14 May 2024 10:46:32 AM
```

2. 并行fft：

num threads = 1:

```bash
xiaoma@xiaoma-virtual-machine:~/Parallel-Programming/PP7$ mpiexec -n 1 fft_parallel.out 
14 May 2024 09:41:52 AM

FFT_SERIAL
  C++ version

  Demonstrate an implementation of the Fast Fourier Transform
  of a complex data vector.

  Accuracy check:

    FFT ( FFT ( X(1:N) ) ) == N * X(1:N)

             N      NITS    Error         Time          Time/Call     MFLOPS

             2     10000   7.85908e-17   0.000783019    3.9151e-08       255.422
             4     10000   1.20984e-16    0.00205443   1.02721e-07       389.403
             8     10000    6.8208e-17    0.00390665   1.95333e-07       614.337
            16     10000   1.43867e-16    0.00939731   4.69865e-07       681.046
            32      1000   1.33121e-16    0.00221595   1.10798e-06       722.037
            64      1000   1.77654e-16    0.00480056   2.40028e-06       799.907
           128      1000   1.92904e-16     0.0114286    5.7143e-06       783.998
           256      1000   2.09232e-16     0.0224046   1.12023e-05       914.096
           512       100   1.92749e-16    0.00468087   2.34044e-05       984.431
          1024       100   2.30861e-16     0.0105499   5.27493e-05        970.63
          2048       100   2.44762e-16     0.0222757   0.000111378       1011.33
          4096       100   2.47978e-16     0.0493331   0.000246665        996.33
          8192        10   2.57809e-16     0.0103927   0.000519635       1024.72
         16384        10   2.73399e-16     0.0223248    0.00111624       1027.45
         32768        10   2.92301e-16     0.0481061    0.00240531       1021.74
         65536        10   2.82993e-16      0.100048     0.0050024       1048.07
        131072         1   3.14967e-16     0.0208368     0.0104184       1069.37
        262144         1    3.2186e-16     0.0449187     0.0224594       1050.47
        524288         1   3.28137e-16     0.0940339     0.0470169       1059.35
       1048576         1    3.2859e-16      0.200343      0.100171       1046.78

FFT_SERIAL:
  Normal end of execution.

14 May 2024 09:41:53 AM
```

num threads = 2:

```bash
xiaoma@xiaoma-virtual-machine:~/Parallel-Programming/PP7$ mpiexec -n 2 fft_parallel.out 
14 May 2024 09:41:57 AM

FFT_SERIAL
  C++ version

  Demonstrate an implementation of the Fast Fourier Transform
  of a complex data vector.

  Accuracy check:

    FFT ( FFT ( X(1:N) ) ) == N * X(1:N)

             N      NITS    Error         Time          Time/Call     MFLOPS

             2     10000   7.85908e-17    0.00446046   2.23023e-07       44.8384
             4     10000   1.20984e-16     0.0224053   1.12027e-06       35.7058
             8     10000    6.8208e-17     0.0442391   2.21195e-06       54.2507
            16     10000   1.43867e-16     0.0617443   3.08721e-06       103.653
            32      1000   1.33121e-16    0.00868105   4.34053e-06       184.309
            64      1000   1.77654e-16     0.0126577   6.32884e-06       303.373
           128      1000   1.92904e-16      0.018527   9.26349e-06       483.619
           256      1000   2.09232e-16     0.0343492   1.71746e-05       596.229
           512       100   1.92749e-16    0.00613537   3.06769e-05       751.054
          1024       100   2.30861e-16     0.0189065   9.45324e-05       541.613
          2048       100   2.44762e-16     0.0281875   0.000140938       799.219
          4096       100   2.47978e-16     0.0502274   0.000251137       978.589
          8192        10   2.57809e-16    0.00923917   0.000461959       1152.66
         16384        10   2.73399e-16     0.0188393   0.000941965       1217.54
         32768        10   2.92301e-16      0.037744     0.0018872       1302.25
         65536        10   2.82993e-16     0.0799977    0.00399988       1310.76
        131072         1   3.14967e-16     0.0170996     0.0085498       1303.09
        262144         1    3.2186e-16     0.0437632     0.0218816       1078.21
        524288         1   3.28137e-16      0.110509     0.0552545       901.418
       1048576         1    3.2859e-16      0.261783      0.130891       801.104

FFT_SERIAL:
  Normal end of execution.

14 May 2024 09:41:59 AM
```

num threads = 4:

```bash
xiaoma@xiaoma-virtual-machine:~/Parallel-Programming/PP7$ mpiexec -n 4 fft_parallel.out 
14 May 2024 09:42:10 AM

FFT_SERIAL
  C++ version

  Demonstrate an implementation of the Fast Fourier Transform
  of a complex data vector.

  Accuracy check:

    FFT ( FFT ( X(1:N) ) ) == N * X(1:N)

             N      NITS    Error         Time          Time/Call     MFLOPS

             2     10000   7.85908e-17    0.00627526   3.13763e-07       31.8712
             4     10000   1.20984e-16     0.0295052   1.47526e-06       27.1138
             8     10000    6.8208e-17     0.0578905   2.89453e-06       41.4576
            16     10000   1.43867e-16     0.0896759    4.4838e-06       71.3681
            32      1000   1.33121e-16      0.012154   6.07698e-06       131.644
            64      1000   1.77654e-16     0.0177595   8.87977e-06       216.222
           128      1000   1.92904e-16     0.0250519    1.2526e-05       357.657
           256      1000   2.09232e-16     0.0419576   2.09788e-05       488.112
           512       100   1.92749e-16     0.0062517   3.12585e-05       737.079
          1024       100   2.30861e-16     0.0143033   7.15166e-05       715.918
          2048       100   2.44762e-16     0.0458389   0.000229195        491.46
          4096       100   2.47978e-16     0.0770827   0.000385413       637.653
          8192        10   2.57809e-16     0.0111179   0.000555894        957.88
         16384        10   2.73399e-16     0.0208595    0.00104298       1099.62
         32768        10   2.92301e-16     0.0369885    0.00184942       1328.85
         65536        10   2.82993e-16     0.0865341     0.0043267       1211.75
        131072         1   3.14967e-16     0.0237039     0.0118519       940.025
        262144         1    3.2186e-16     0.0553872     0.0276936       851.928
        524288         1   3.28137e-16       0.11854     0.0592698       840.349
       1048576         1    3.2859e-16      0.387482      0.193741       541.226

FFT_SERIAL:
  Normal end of execution.

14 May 2024 09:42:13 AM
```

num threads = 8:

```bash
xiaoma@xiaoma-virtual-machine:~/Parallel-Programming/PP7$ mpiexec -n 8 fft_parallel.out 
14 May 2024 09:42:17 AM

FFT_SERIAL
  C++ version

  Demonstrate an implementation of the Fast Fourier Transform
  of a complex data vector.

  Accuracy check:

    FFT ( FFT ( X(1:N) ) ) == N * X(1:N)

             N      NITS    Error         Time          Time/Call     MFLOPS

             2     10000   7.85908e-17     0.0772112   3.86056e-06        2.5903
             4     10000   1.20984e-16      0.100456   5.02281e-06       7.96366
             8     10000    6.8208e-17      0.154613   7.73065e-06       15.5226
            16     10000   1.43867e-16      0.180841   9.04204e-06       35.3902
            32      1000   1.33121e-16     0.0324657   1.62328e-05       49.2829
            64      1000   1.77654e-16      0.139166   6.95831e-05       27.5929
           128      1000   1.92904e-16      0.142666   7.13328e-05       62.8042
           256      1000   2.09232e-16      0.138678   6.93389e-05        147.68
           512       100   1.92749e-16     0.0257602   0.000128801       178.881
          1024       100   2.30861e-16     0.0275292   0.000137646       371.969
          2048       100   2.44762e-16     0.0955982   0.000477991       235.653
          4096       100   2.47978e-16      0.185759   0.000928796       264.601
          8192        10   2.57809e-16     0.0195989   0.000979946       543.377
         16384        10   2.73399e-16     0.0842156    0.00421078       272.368
         32768        10   2.92301e-16      0.127078    0.00635389       386.787
         65536        10   2.82993e-16      0.148438    0.00742191       706.406
        131072         1   3.14967e-16     0.0559537     0.0279768       398.227
        262144         1    3.2186e-16     0.0596932     0.0298466       790.474
        524288         1   3.28137e-16      0.135923     0.0679616       732.875
       1048576         1    3.2859e-16      0.697102      0.348551       300.839

FFT_SERIAL:
  Normal end of execution.

14 May 2024 09:42:22 AM
```

num threads = 16:

```bash
xiaoma@xiaoma-virtual-machine:~/Parallel-Programming/PP7$ mpiexec -n 16 fft_parallel.out 
14 May 2024 09:42:33 AM

FFT_SERIAL
  C++ version

  Demonstrate an implementation of the Fast Fourier Transform
  of a complex data vector.

  Accuracy check:

    FFT ( FFT ( X(1:N) ) ) == N * X(1:N)

             N      NITS    Error         Time          Time/Call     MFLOPS

             2     10000   7.85908e-17      0.138398   6.91992e-06        1.4451
             4     10000   1.20984e-16       0.18058   9.02902e-06       4.43016
             8     10000    6.8208e-17      0.614204   3.07102e-05        3.9075
            16     10000   1.43867e-16      0.985343   4.92672e-05        6.4952
            32      1000   1.33121e-16      0.305703   0.000152852       5.23383
            64      1000   1.77654e-16      0.272861   0.000136431       14.0731
           128      1000   1.92904e-16      0.276147   0.000138073       32.4465
           256      1000   2.09232e-16      0.308307   0.000154153       66.4273
           512       100   1.92749e-16     0.0775561    0.00038778       59.4151
          1024       100   2.30861e-16      0.103621   0.000518104       98.8218
          2048       100   2.44762e-16      0.165978   0.000829892       135.728
          4096       100   2.47978e-16      0.178249   0.000891246       275.749
          8192        10   2.57809e-16      0.108035    0.00540175       98.5755
         16384        10   2.73399e-16      0.128036    0.00640179        179.15
         32768        10   2.92301e-16      0.108994    0.00544972       450.959
         65536        10   2.82993e-16      0.339911     0.0169955       308.486
        131072         1   3.14967e-16     0.0389044     0.0194522       572.744
        262144         1    3.2186e-16      0.135822     0.0679111        347.41
        524288         1   3.28137e-16      0.192497     0.0962486       517.487
       1048576         1    3.2859e-16       1.86979      0.934895        112.16

FFT_SERIAL:
  Normal end of execution.

14 May 2024 09:42:44 AM
```

3. 数据打包：

num threads = 1:

```bash
xiaoma@xiaoma-virtual-machine:~/Parallel-Programming/PP7$ mpiexec -n 1 fft_parallel.out 
14 May 2024 10:52:32 AM

FFT_SERIAL
  C++ version

  Demonstrate an implementation of the Fast Fourier Transform
  of a complex data vector.

  Accuracy check:

    FFT ( FFT ( X(1:N) ) ) == N * X(1:N)

             N      NITS    Error         Time          Time/Call     MFLOPS

             2     10000   7.85908e-17    0.00104893   5.24464e-08       190.671
             4     10000   1.20984e-16    0.00220839    1.1042e-07       362.254
             8     10000    6.8208e-17    0.00447432   2.23716e-07       536.395
            16     10000   1.43867e-16     0.0111906    5.5953e-07       571.909
            32      1000   1.33121e-16    0.00212011   1.06005e-06       754.679
            64      1000   1.77654e-16    0.00487557   2.43779e-06         787.6
           128      1000   1.92904e-16     0.0102711   5.13554e-06       872.352
           256      1000   2.09232e-16     0.0229712   1.14856e-05       891.552
           512       100   1.92749e-16    0.00483739    2.4187e-05       952.579
          1024       100   2.30861e-16     0.0136653   6.83265e-05       749.343
          2048       100   2.44762e-16     0.0242478   0.000121239       929.075
          4096       100   2.47978e-16     0.0566333   0.000283166         867.9
          8192        10   2.57809e-16     0.0110441   0.000552204       964.282
         16384        10   2.73399e-16     0.0233009    0.00116504       984.409
         32768        10   2.92301e-16     0.0482299    0.00241149       1019.12
         65536        10   2.82993e-16      0.110474     0.0055237       949.162
        131072         1   3.14967e-16     0.0212037     0.0106019       1050.86
        262144         1    3.2186e-16     0.0451716     0.0225858       1044.59
        524288         1   3.28137e-16     0.0938542     0.0469271       1061.38
       1048576         1    3.2859e-16      0.204913      0.102457       1023.43

FFT_SERIAL:
  Normal end of execution.

14 May 2024 10:52:33 AM
```

num threads = 2:

```bash
xiaoma@xiaoma-virtual-machine:~/Parallel-Programming/PP7$ mpiexec -n 2 fft_parallel.out 
14 May 2024 10:52:49 AM

FFT_SERIAL
  C++ version

  Demonstrate an implementation of the Fast Fourier Transform
  of a complex data vector.

  Accuracy check:

    FFT ( FFT ( X(1:N) ) ) == N * X(1:N)

             N      NITS    Error         Time          Time/Call     MFLOPS

             2     10000   7.85908e-17    0.00316984   1.58492e-07       63.0947
             4     10000   1.20984e-16     0.0227772   1.13886e-06       35.1228
             8     10000    6.8208e-17     0.0386806   1.93403e-06       62.0466
            16     10000   1.43867e-16     0.0605524   3.02762e-06       105.694
            32      1000   1.33121e-16     0.0101196   5.05982e-06       158.108
            64      1000   1.77654e-16     0.0129869   6.49344e-06       295.683
           128      1000   1.92904e-16     0.0207122   1.03561e-05       432.596
           256      1000   2.09232e-16     0.0380032   1.90016e-05       538.902
           512       100   1.92749e-16    0.00841365   4.20683e-05       547.681
          1024       100   2.30861e-16     0.0143221   7.16104e-05        714.98
          2048       100   2.44762e-16     0.0264642   0.000132321       851.264
          4096       100   2.47978e-16     0.0535224   0.000267612       918.345
          8192        10   2.57809e-16     0.0106453   0.000532266        1000.4
         16384        10   2.73399e-16     0.0221982    0.00110991       1033.31
         32768        10   2.92301e-16      0.055875    0.00279375       879.678
         65536        10   2.82993e-16      0.103572    0.00517859       1012.41
        131072         1   3.14967e-16      0.023774      0.011887       937.252
        262144         1    3.2186e-16     0.0777206     0.0388603       607.122
```

num threads = 4:

```bash
xiaoma@xiaoma-virtual-machine:~/Parallel-Programming/PP7$ mpiexec -n 4 fft_parallel.out 
14 May 2024 10:53:11 AM

FFT_SERIAL
  C++ version

  Demonstrate an implementation of the Fast Fourier Transform
  of a complex data vector.

  Accuracy check:

    FFT ( FFT ( X(1:N) ) ) == N * X(1:N)

             N      NITS    Error         Time          Time/Call     MFLOPS

             2     10000   7.85908e-17    0.00566759    2.8338e-07       35.2883
             4     10000   1.20984e-16     0.0253054   1.26527e-06       31.6139
             8     10000    6.8208e-17     0.0572052   2.86026e-06       41.9542
            16     10000   1.43867e-16     0.0909117   4.54558e-06        70.398
            32      1000   1.33121e-16     0.0158339   7.91695e-06       101.049
            64      1000   1.77654e-16     0.0167437   8.37186e-06        229.34
           128      1000   1.92904e-16     0.0235728   1.17864e-05         380.1
           256      1000   2.09232e-16     0.0398969   1.99484e-05       513.324
           512       100   1.92749e-16    0.00701936   3.50968e-05        656.47
          1024       100   2.30861e-16     0.0214153   0.000107077       478.162
          2048       100   2.44762e-16     0.0365011   0.000182506       617.186
          4096       100   2.47978e-16     0.0627412   0.000313706       783.409
          8192        10   2.57809e-16     0.0112273   0.000561366       948.544
         16384        10   2.73399e-16     0.0231676    0.00115838       990.072
         32768        10   2.92301e-16     0.0454107    0.00227053       1082.39
         65536        10   2.82993e-16     0.0935071    0.00467536       1121.39
        131072         1   3.14967e-16     0.0267504     0.0133752       832.969
        262144         1    3.2186e-16     0.0734753     0.0367377       642.201
        524288         1   3.28137e-16      0.207254      0.103627       480.641
```

num threads = 8:

```bash
xiaoma@xiaoma-virtual-machine:~/Parallel-Programming/PP7$ mpiexec -n 8 fft_parallel.out 
14 May 2024 10:53:52 AM

FFT_SERIAL
  C++ version

  Demonstrate an implementation of the Fast Fourier Transform
  of a complex data vector.

  Accuracy check:

    FFT ( FFT ( X(1:N) ) ) == N * X(1:N)

             N      NITS    Error         Time          Time/Call     MFLOPS

             2     10000   7.85908e-17     0.0589881    2.9494e-06       3.39052
             4     10000   1.20984e-16       0.08248     4.124e-06       9.69932
             8     10000    6.8208e-17      0.112283   5.61415e-06       21.3746
            16     10000   1.43867e-16      0.183209   9.16044e-06       34.9328
            32      1000   1.33121e-16     0.0172138   8.60692e-06       92.9484
            64      1000   1.77654e-16     0.0923176   4.61588e-05       41.5955
           128      1000   1.92904e-16     0.0937695   4.68847e-05       95.5535
           256      1000   2.09232e-16     0.0997728   4.98864e-05       205.266
           512       100   1.92749e-16     0.0083796    4.1898e-05       549.907
          1024       100   2.30861e-16       0.03501    0.00017505       292.488
          2048       100   2.44762e-16      0.121339   0.000606694       185.662
          4096       100   2.47978e-16      0.151344   0.000756718       324.771
          8192        10   2.57809e-16      0.017878   0.000893901       595.681
         16384        10   2.73399e-16     0.0517639    0.00258819        443.12
         32768        10   2.92301e-16     0.0896368    0.00448184       548.346
         65536        10   2.82993e-16      0.190532     0.0095266       550.341
        131072         1   3.14967e-16     0.0328873     0.0164437       677.533
        262144         1    3.2186e-16     0.0890415     0.0445207       529.932
        524288         1   3.28137e-16      0.208615      0.104307       477.506
```

num threads = 16:

```bash
xiaoma@xiaoma-virtual-machine:~/Parallel-Programming/PP7$ mpiexec -n 16 fft_parallel.out 
14 May 2024 10:54:00 AM

FFT_SERIAL
  C++ version

  Demonstrate an implementation of the Fast Fourier Transform
  of a complex data vector.

  Accuracy check:

    FFT ( FFT ( X(1:N) ) ) == N * X(1:N)

             N      NITS    Error         Time          Time/Call     MFLOPS

             2     10000   7.85908e-17      0.124572   6.22862e-06       1.60549
             4     10000   1.20984e-16      0.325478   1.62739e-05       2.45793
             8     10000    6.8208e-17      0.421314   2.10657e-05       5.69646
            16     10000   1.43867e-16      0.722421   3.61211e-05        8.8591
            32      1000   1.33121e-16      0.167957   8.39783e-05       9.52627
            64      1000   1.77654e-16      0.214798   0.000107399       17.8773
           128      1000   1.92904e-16      0.243885   0.000121942       36.7387
           256      1000   2.09232e-16      0.341548   0.000170774       59.9623
           512       100   1.92749e-16     0.0853835   0.000426917       53.9683
          1024       100   2.30861e-16      0.109655   0.000548275       93.3838
          2048       100   2.44762e-16       0.13979    0.00069895       161.156
          4096       100   2.47978e-16      0.360017    0.00180009       136.527
          8192        10   2.57809e-16     0.0941126    0.00470563       113.158
         16384        10   2.73399e-16      0.123298    0.00616489       186.034
         32768        10   2.92301e-16      0.106396    0.00531982        461.97
         65536        10   2.82993e-16      0.247138     0.0123569       424.287
        131072         1   3.14967e-16     0.0576358     0.0288179       386.604
        262144         1    3.2186e-16      0.160802     0.0804012        293.44
        524288         1   3.28137e-16      0.278267      0.139134       357.982
       1048576         1    3.2859e-16       1.82497      0.924562        124.35

FFT_SERIAL:
  Normal end of execution.

14 May 2024 10:55:29 AM
```

### 4.1 正确性

在测试运行效率和内存消耗前，我们先来验证一下并行结果的准确性。

比较串行和并行的运行结果，我们可以看到，N取2到1048576，Error串行与并行都相等。因此，我们实验的正确性是有保障的。

### 4.2 运行效率

悲观的是，虽然正确性是可观的。但是，并行的运行效率却没有串行那般高。主要原因是由于进程间的通信导致。可以看到，我在step中有多达5对的MPI_Send和MPI_Recv函数，这严重导致了运行效率的降低。因此，观察我的实验结果，当线程数设置为1时，与串行结果运行效率基本一致；然而，随着线程数的提高，运行效率却越来越低。

我尝试用打包的方式来减小通信的消耗。观察结果可知，打包后程序的运行效率确实是有所提升的。

打包后的程序运行效率比普通的并行版本好的主要原因在于减少了MPI通信的开销。
在普通的并行版本中，每个数组需要单独发送和接收：

```cpp
MPI_Send(&a[start * mj2], (end - start) * mj2, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
MPI_Send(&b[start * mj2], (end - start) * mj2, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
MPI_Send(&w[start * mj2], (end - start) * mj2, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);
```

这意味着每次通信操作都需要进行三次发送和接收，对于每个进程的每个通信，这将导致多次的通信操作。

在打包版本中，所有的数据都被打包成一个大的数据块：

```cpp
double sendbuf[count * 3];
int position = 0;
MPI_Pack(&a[start * mj2], count, MPI_DOUBLE, sendbuf, count * 3 * sizeof(MPI_DOUBLE), &position, MPI_COMM_WORLD);
MPI_Pack(&b[start * mj2], count, MPI_DOUBLE, sendbuf, count * 3 * sizeof(MPI_DOUBLE), &position, MPI_COMM_WORLD);
MPI_Pack(&w[start * mj2], count, MPI_DOUBLE, sendbuf, count * 3 * sizeof(MPI_DOUBLE), &position, MPI_COMM_WORLD);
MPI_Send(sendbuf, position, MPI_PACKED, i, 0, MPI_COMM_WORLD);
```

这样就将所有需要发送的数据通过一次通信操作发送出去，同样接收时也是一次性接收，这样显著减少了通信次数。

### 4.3 内存消耗

1. 串行：

```bash
PP7 ) valgrind --tool=massif --stacks=yes ./fft_serial.out 
```



<img src="https://github.com/xiao10ma/Parallel-Programming/blob/master/PP7/valgrind.png?raw=true" style="zoom: 67%;" />

```bash
PP7 ) ms_print massif.out.18869 
```

<img src="https://github.com/xiao10ma/Parallel-Programming/blob/master/PP7/mem_serial.png?raw=true" style="zoom: 67%;" />

2. 并行：

<img src="https://github.com/xiao10ma/Parallel-Programming/blob/master/PP7/mem_parallel.png?raw=true" style="zoom:67%;" />

3. 打包并行：

<img src="https://github.com/xiao10ma/Parallel-Programming/blob/master/PP7/mem_parallel_pack.png?raw=true" style="zoom:67%;" />

通过分析内存消耗图表，可以得出以下结论：

1. 并行版本的内存消耗高于串行版本，主要原因是进程间的通信和数据分配带来了额外的内存开销。
2. 数据打包优化在内存消耗上的效果不显著，但有助于提高通信效率和整体性能。

## 5. 实验感想

在这次实验中，我学习了如何使用MPI对快速傅里叶变换（FFT）进行并行化处理。通过将串行代码转换为并行版本，并使用MPI_Pack和MPI_Unpack函数对数据进行打包和解包，我成功地减少了通信次数，提高了程序的性能。虽然并行版本在进程数较少时表现良好，但随着进程数的增加，通信开销显著增加，导致整体性能下降。

通过这次实验，我对并行计算有了更深的理解。尽管并行计算能加速任务处理，但进程间的通信开销是一个重要的瓶颈，需要仔细优化。总的来说，这次实验不仅帮助我掌握了MPI的基本使用，还让我体会到并行计算的复杂性和挑战性，为我今后的学习和研究打下了坚实的基础。
